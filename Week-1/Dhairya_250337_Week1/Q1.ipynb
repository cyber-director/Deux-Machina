{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "DdLUP0vipA72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
        "\n",
        "\n",
        "random_map = generate_random_map(size=4, p=0.8)\n",
        "print(\"Generated Map:\")\n",
        "for row in random_map:\n",
        "  print(row)\n",
        "\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=True)\n",
        "\n",
        "\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "episodes = 20000\n",
        "max_steps = 100\n",
        "eps_start = 1.0\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.999\n",
        "\n",
        "\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "rewards_all_episodes = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXQlE45oo9gQ",
        "outputId": "770dab5c-ae34-4ea3-cd23-5f97967b2e1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Map:\n",
            "SHFF\n",
            "FHHF\n",
            "FFFF\n",
            "FFHG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "UT_uFC_io-xQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErmrnzTPUAEw",
        "outputId": "de95805f-2066-4b96-edfd-dea491c9cf1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started\n",
            "Episode 0, epsilon=1.0000\n",
            "s=0, a=1, r=0, s'=4, Q=0.005\n",
            "s=4, a=1, r=0, s'=5, Q=0.001\n",
            "Episode 2000, epsilon=0.1352\n",
            "Episode 4000, epsilon=0.0183\n",
            "s=0, a=1, r=0, s'=1, Q=0.000\n",
            "Episode 6000, epsilon=0.0100\n",
            "Episode 8000, epsilon=0.0100\n",
            "Episode 10000, epsilon=0.0100\n",
            "s=0, a=0, r=0, s'=0, Q=0.037\n",
            "s=0, a=0, r=0, s'=0, Q=0.036\n",
            "s=0, a=0, r=0, s'=0, Q=0.034\n",
            "Episode 12000, epsilon=0.0100\n",
            "Episode 14000, epsilon=0.0100\n",
            "s=0, a=0, r=0, s'=4, Q=0.174\n",
            "s=4, a=0, r=0, s'=8, Q=0.098\n",
            "s=8, a=1, r=0, s'=12, Q=0.066\n",
            "Episode 16000, epsilon=0.0100\n",
            "Episode 18000, epsilon=0.0100\n",
            "Training finished\n",
            "[[5.14106814e-02 1.02367361e-03 2.06103398e-03 2.45600251e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.67580853e-02]\n",
            " [6.33998233e-02 4.10783082e-03 1.02751402e-01 5.04970762e-02]\n",
            " [2.28973939e-01 1.77159951e-04 2.79837170e-03 1.48010393e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.17932815e-02 4.20210447e-03 4.76484397e-01 6.73571856e-03]\n",
            " [3.35913249e-03 2.27175159e-03 4.82074895e-02 2.18038975e-03]\n",
            " [2.80930496e-05 1.86687658e-01 4.80220415e-04 8.02441642e-04]\n",
            " [1.47381680e-05 1.42431964e-01 1.17024238e-06 6.54424966e-06]\n",
            " [2.87623864e-01 9.87409530e-01 2.04763725e-01 1.28858256e-01]\n",
            " [1.49073279e-02 1.50005454e-02 1.87667082e-02 1.39693467e-02]\n",
            " [4.88566424e-03 1.53914466e-03 1.08696501e-03 2.51824585e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Q-table saved\n"
          ]
        }
      ],
      "source": [
        "print(\"Training started\")\n",
        "\n",
        "\n",
        "for episode in range(episodes):\n",
        "  state, _ = env.reset()\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "  epsilon = max(eps_end, eps_start * (eps_decay ** episode))\n",
        "\n",
        "\n",
        "  if episode % 2000 == 0:\n",
        "    print(f\"Episode {episode}, epsilon={epsilon:.4f}\")\n",
        "\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = np.argmax(Q[state])\n",
        "\n",
        "\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "\n",
        "    old_value = Q[state, action]\n",
        "    next_max = np.max(Q[next_state])\n",
        "    Q[state, action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "\n",
        "    if episode % 5000 == 0 and step < 3:\n",
        "      print(f\"s={state}, a={action}, r={reward}, s'={next_state}, Q={Q[state, action]:.3f}\")\n",
        "\n",
        "\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  rewards_all_episodes.append(total_reward)\n",
        "\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n",
        "\n",
        "np.save(\"qtable_random_frozenlake.npy\", Q)\n",
        "\n",
        "\n",
        "with open(\"qtable_random_frozenlake.pkl\", \"wb\") as f:\n",
        "  pickle.dump(Q, f)\n",
        "\n",
        "print(Q)\n",
        "print(\"Q-table saved\")\n",
        "\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=True, render_mode=\"human\")\n",
        "state, _ = env.reset()\n",
        "done = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "xn0Sn08So5yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing started\")\n",
        "\n",
        "\n",
        "for step in range(50):\n",
        "  action = np.argmax(Q[state])\n",
        "  next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "  print(f\"step={step}, state={state}, action={action}, reward={reward}\")\n",
        "  state = next_state\n",
        "  if terminated or truncated:\n",
        "    print(\"Success\" if reward == 1 else \"Failure\")\n",
        "    break\n",
        "\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odylt3nwo4GP",
        "outputId": "e2dbd582-3d5f-405e-df66-f2770fe6e285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing started\n",
            "step=0, state=0, action=0, reward=0\n",
            "step=1, state=0, action=0, reward=0\n",
            "step=2, state=4, action=2, reward=0\n",
            "step=3, state=0, action=0, reward=0\n",
            "step=4, state=0, action=0, reward=0\n",
            "step=5, state=0, action=0, reward=0\n",
            "step=6, state=0, action=0, reward=0\n",
            "step=7, state=4, action=2, reward=0\n",
            "step=8, state=8, action=2, reward=0\n",
            "step=9, state=4, action=2, reward=0\n",
            "step=10, state=0, action=0, reward=0\n",
            "step=11, state=0, action=0, reward=0\n",
            "step=12, state=4, action=2, reward=0\n",
            "step=13, state=0, action=0, reward=0\n",
            "step=14, state=0, action=0, reward=0\n",
            "step=15, state=0, action=0, reward=0\n",
            "step=16, state=4, action=2, reward=0\n",
            "step=17, state=5, action=1, reward=0\n",
            "step=18, state=9, action=0, reward=0\n",
            "step=19, state=13, action=2, reward=0\n",
            "step=20, state=9, action=0, reward=0\n",
            "step=21, state=8, action=2, reward=0\n",
            "step=22, state=9, action=0, reward=0\n",
            "step=23, state=13, action=2, reward=0\n",
            "step=24, state=14, action=1, reward=1\n",
            "Success\n"
          ]
        }
      ]
    }
  ]
}